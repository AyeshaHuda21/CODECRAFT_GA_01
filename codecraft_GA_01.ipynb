{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AyeshaHuda21/CODECRAFT_GA_01/blob/main/codecraft_GA_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBWibHIpFxVl"
      },
      "source": [
        "# Task 01 – Text Generation with GPT-2  \n",
        "**Internship: Generative AI (Code Craft)**  \n",
        "\n",
        "In this notebook, I explore **text generation** using GPT-2, a transformer model developed by OpenAI.  \n",
        "\n",
        "The task has three main goals:  \n",
        "1. Run the **pre-trained GPT-2 model** to generate text.  \n",
        "2. **Fine-tune GPT-2** on a custom dataset that I created.  \n",
        "3. Experiment with **different generation techniques** (temperature, top-k, top-p) to control creativity.  \n",
        "\n",
        "This exercise helped me understand the basics of **Generative AI in NLP** and how models can be adapted to new writing styles.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zO6aArh7F883"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets torch --quiet\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from datasets import load_dataset\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xN4nptt1GTMp"
      },
      "outputs": [],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "prompt = \"Generative AI will change the future because\"\n",
        "inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "outputs = model.generate(inputs, max_length=50, num_return_sequences=1, temperature=0.7)\n",
        "\n",
        "print(\"Prompt:\", prompt)\n",
        "print(\"Generated Text:\\n\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAchWafLGykE"
      },
      "source": [
        "#**3.Custom dataset creation**\n",
        "    Here we make a small unique dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fxEPKeIGQdQ"
      },
      "outputs": [],
      "source": [
        "custom_text = \"\"\"\n",
        "Artificial Intelligence is not just about automation, it is about augmentation.\n",
        "Generative AI gives machines the power to imagine, just like humans.\n",
        "Every breakthrough in AI begins with curiosity and persistence.\n",
        "The future belongs to those who combine creativity with technology.\n",
        "Code Craft interns are building the next wave of AI innovation.\n",
        "Learning by doing is the best way to master Generative AI.\n",
        "AI should not replace humans, but help humans achieve more.\n",
        "\"\"\"\n",
        "\n",
        "with open(\"custom_dataset.txt\", \"w\") as f:\n",
        "    f.write(custom_text)\n",
        "\n",
        "dataset = load_dataset(\"text\", data_files={\"train\": \"custom_dataset.txt\"})\n",
        "dataset[\"train\"][0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "araAyXsZHjSP"
      },
      "source": [
        "##**4.Tokenize dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "000cGvRJGMrE"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aa_OR50kHv_Z"
      },
      "source": [
        "##**5.Fine-tune GPT-02**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPFQlMITGNFE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "from transformers import DataCollatorForLanguageModeling, GPT2Tokenizer, GPT2LMHeadModel, TrainingArguments, Trainer\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load and tokenize the dataset\n",
        "custom_text = \"\"\"\n",
        "Artificial Intelligence is not just about automation, it is about augmentation.\n",
        "Generative AI gives machines the power to imagine, just like humans.\n",
        "Every breakthrough in AI begins with curiosity and persistence.\n",
        "The future belongs to those who combine creativity with technology.\n",
        "Code Craft interns are building the next wave of AI innovation.\n",
        "Learning by doing is the best way to master Generative AI.\n",
        "AI should not replace humans, but help humans achieve more.\n",
        "\"\"\"\n",
        "\n",
        "with open(\"custom_dataset.txt\", \"w\") as f:\n",
        "    f.write(custom_text)\n",
        "\n",
        "dataset = load_dataset(\"text\", data_files={\"train\": \"custom_dataset.txt\"})\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token # Set pad token\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\")\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    save_steps=200,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGY7HbFdIFAi"
      },
      "source": [
        "##**6.Generate text after fine tuning**\n",
        "\n",
        ">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MZmAFgmEbt7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c931dce2-0612-4c4e-cefe-3034a88e3967"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: In the future, Generative AI\n",
            "Generated Text:\n",
            " In the future, Generative AI is going to be a huge step forward.\n",
            "\n",
            "The next generation of AI is going to be a huge step forward in human intelligence.\n",
            "\n",
            "AI is going to be a huge step forward in human intelligence.\n",
            "\n",
            "We are going to be able to create\n"
          ]
        }
      ],
      "source": [
        "prompt = \"In the future, Generative AI\"\n",
        "inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "outputs = model.generate(inputs, max_length=60, num_return_sequences=1, temperature=0.7)\n",
        "\n",
        "print(\"Prompt:\", prompt)\n",
        "print(\"Generated Text:\\n\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZaIp2Wrb_x2"
      },
      "source": [
        "##**7.Advanced Generation Experiments**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "c518PMLicKOM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeb32c96-68d8-4465-d954-478b48c6a327"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Default:\n",
            " The role of Generative AI in the future is not to automate tomorrow. It is to help us become smarter.\n",
            "\n",
            "Generative AI is a paradigm shift in science. Today, the goal of AI is to eliminate human error, improve human interaction, and create new forms of personal, creative,\n",
            "\n",
            "High Creativity (temp=1.0):\n",
            " The role of Generative AI in the future is well known. A few years ago, AI was touted as the next advance in artificial intelligence.\n",
            "\n",
            "But it's not the brightest AI revolution ever thought to move the world. In fact, it only works in ways we already know how:\n",
            "\n",
            "\n",
            "Top-K (k=50):\n",
            " The role of Generative AI in the future is not just to create a better world, but to create a new one. In this chapter, we will explore the possibility of Artificial Intelligence (AI) as a viable innovation that will change the way we think about technology, and how we can help it\n",
            "\n",
            "Top-P (p=0.9):\n",
            " The role of Generative AI in the future is to enable robots to solve many tasks. It will help us to solve the problems we face today.\n",
            "\n",
            "The Future of Artificial Intelligence\n",
            "\n",
            "AI is an ever-evolving technology. It will change the way we think, act and imagine.\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "prompt = \"The role of Generative AI in the future is\"\n",
        "\n",
        "# Create a pipeline for text generation\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Default\n",
        "output1 = generator(prompt, max_new_tokens=50, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
        "print(\"\\nDefault:\\n\", output1[0]['generated_text'])\n",
        "\n",
        "# High creativity (temperature = 1.0)\n",
        "output2 = generator(prompt, max_new_tokens=50, num_return_sequences=1, temperature=1.0 + 1e-8, pad_token_id=tokenizer.eos_token_id)\n",
        "print(\"\\nHigh Creativity (temp=1.0):\\n\", output2[0]['generated_text'])\n",
        "\n",
        "# Top-K sampling\n",
        "output3 = generator(prompt, max_new_tokens=50, num_return_sequences=1, top_k=50, pad_token_id=tokenizer.eos_token_id)\n",
        "print(\"\\nTop-K (k=50):\\n\", output3[0]['generated_text'])\n",
        "\n",
        "# Top-P sampling\n",
        "output4 = generator(prompt, max_new_tokens=50, num_return_sequences=1, top_p=0.9, pad_token_id=tokenizer.eos_token_id)\n",
        "print(\"\\nTop-P (p=0.9):\\n\", output4[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔍 Observations\n",
        "- Default GPT-2 was fluent but generic.\n",
        "- High temperature (1.0) gave more creative, but less consistent outputs.\n",
        "- Top-K sampling produced focused text.\n",
        "- Top-P sampling balanced coherence with creativity best.  \n",
        "\n",
        "## ✅ Conclusion\n",
        "This task taught me how to:\n",
        "1. Use GPT-2 for text generation.  \n",
        "2. Fine-tune it on a custom dataset.  \n",
        "3. Control AI creativity with generation parameters.  \n",
        "\n",
        "It was my first step into **hands-on Generative AI** 🚀.\n"
      ],
      "metadata": {
        "id": "qgsJOGLTxeKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model + tokenizer\n",
        "output_dir = \"./fine_tuned_gpt2\"\n",
        "\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(\"Model saved in\", output_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wbjGqplzwim",
        "outputId": "047960c2-14d4-4ff6-d3bd-bd51b931804c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved in ./fine_tuned_gpt2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\"./fine_tuned_gpt2\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"./fine_tuned_gpt2\")\n"
      ],
      "metadata": {
        "id": "ShybZiar0B_L"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from transformers import pipeline, GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load your fine-tuned model\n",
        "model = GPT2LMHeadModel.from_pretrained(\"./fine_tuned_gpt2\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"./fine_tuned_gpt2\")\n",
        "\n",
        "# Create pipeline\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Define chatbot function\n",
        "def generate_text(prompt, max_length=100, temperature=0.7):\n",
        "    result = generator(\n",
        "        prompt,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    return result[0]['generated_text']\n",
        "\n",
        "# Build Gradio UI\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## 🤖 Fine-Tuned GPT-2 Text Generator\")\n",
        "\n",
        "    with gr.Row():\n",
        "        prompt = gr.Textbox(label=\"Enter a prompt\", placeholder=\"Type something...\")\n",
        "        max_len = gr.Slider(50, 200, value=100, step=10, label=\"Max Length\")\n",
        "        temp = gr.Slider(0.1, 1.5, value=0.7, step=0.1, label=\"Creativity (Temperature)\")\n",
        "\n",
        "    output = gr.Textbox(label=\"Generated Output\")\n",
        "\n",
        "    btn = gr.Button(\"Generate\")\n",
        "    btn.click(fn=generate_text, inputs=[prompt, max_len, temp], outputs=output)\n",
        "\n",
        "# Launch app\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "R3cBvaYB0IY6",
        "outputId": "fe6df187-d50a-474d-f31a-bb6dd3ba4a0a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://3459ff2ed963bef7cf.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3459ff2ed963bef7cf.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b727eee1"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/<AyeshaHuda21>/<codecraft_GA_01.ipynb>/blob/main/<codecraft_GA_O1>.ipynb)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOT51cD4C8spR2z5/GmRFh0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}